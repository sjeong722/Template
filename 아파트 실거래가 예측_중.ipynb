{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba74d64",
   "metadata": {},
   "source": [
    "## 아파트 실거래가 예측 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e0330",
   "metadata": {},
   "source": [
    "## Stage 1. 데이터확인\n",
    "데이터를 확인해 보고 어떻게 분석할지 계획해 봅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbe1ed",
   "metadata": {},
   "source": [
    "### 1. 데이터 분석 전 준비\n",
    "[문제1]\n",
    "CSV 파일을 데이터프레임 형식으로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f691f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255df7c0",
   "metadata": {},
   "source": [
    "### 2. 데이터 확인\n",
    "[문제2]\n",
    "train의 인덱스 기준 상위 5개 데이터 확인하기.\n",
    "그리고 앞으로 어떻게 분석을 진행할지 생각해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3dbb1",
   "metadata": {},
   "source": [
    "### 3. 데이터 타입 확인\n",
    "학습 데이터를 이용해서 머신러닝 모델을 학습 시키기 위해서는,\n",
    "데이터의 타입(Dtype)이 계산 가능한 형태여야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c092215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c02966",
   "metadata": {},
   "source": [
    "### 4. 데이터 통계값 확인\n",
    "[문제3] describe() 함수를 이용해 수치형 데이터의 통계값을 확인해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47840314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(string):\n",
    "    if type(string) == str:\n",
    "        string = string.replace(',','')\n",
    "        return int(string)\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "train['transaction_real_price'] = train['transaction_real_price'].apply(str_to_int)\n",
    "\n",
    "columns = ['exclusive_use_area', 'floor', 'transaction_real_price']\n",
    "train[columns].describe()\n",
    "\n",
    "# 평균 mean값과 표준편차 std값을 통해 가격 분포 확인\n",
    "# 1사분위수 25% 값과 2사분위수 50% 값 차이가 미미한 것으로 보아\n",
    "# exclusive_use_area 는 좁은 범위에 몰려있음을 알 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84ab19",
   "metadata": {},
   "source": [
    "### 5. 테스트 데이터 확인\n",
    "[문제4] \n",
    "test의 인덱스 기준 상위 10개 데이터 확인."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e0943",
   "metadata": {},
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8108e",
   "metadata": {},
   "source": [
    "### 6. 제출 데이터 확인\n",
    "[문제5]\n",
    "submission의 인덱스 기준 상위 10개 데이터 확인. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643793f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0cbae",
   "metadata": {},
   "source": [
    "### 7. 타겟의 평균값 구하기\n",
    "[문제6] \n",
    "아파트 실거래가의 평균 값을 구하고, 그 값을 submission에 적용하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(string):\n",
    "    if type(string) == str:\n",
    "        string = string.replace(',','')\n",
    "        return int(string)\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "train['transaction_real_price'] = train['transaction_real_price'].apply(str_to_int)\n",
    "\n",
    "mean_apt_price = round(train['transaction_real_price'].mean())\n",
    "submission['transaction_real_price'] = mean_apt_price\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e18842",
   "metadata": {},
   "source": [
    "### 8. 제출 파일 생성\n",
    "[문제7]\n",
    "예측을 담은 submission을 CSV 파일로 생성하여 제출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a557116",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a883bcd",
   "metadata": {},
   "source": [
    "## Stage2. 탐색적 데이터 분석(EDA)\n",
    "목표: 데이터 탐색을 통한 데이터 분석 방향 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799780e4",
   "metadata": {},
   "source": [
    "### 1. 데이터 분석 전 준비\n",
    "[문제1]\n",
    "데이터 분석을 위해서는, 먼저 데이터를 볼 수 있어야 함.\n",
    "학습용 데이터를 읽어와서 train 변수 안에 넣기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e790544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ff843",
   "metadata": {},
   "source": [
    "### 2. 타겟 데이터 살펴보기\n",
    "[문제2] \n",
    "str_to_int 함수를 만들어 자료형이 str로 되어있는 'transaction_real_price'데이터를 int로 변경하기.\n",
    "그리고 숫자 사이에 들어있는 ','(콤마) 제거하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def str_to_int(string):\n",
    "    if type(string) == str;\n",
    "        string = string.replace(',','')\n",
    "        return int(string)\n",
    "    else: \n",
    "        return string\n",
    "train['transaction_real_price'] = train['transaction_real_price'].apply(str_to_int)\n",
    "data = train['transaction_real_price']\n",
    "\n",
    "# Seaborn을 사용하여 히스토그램 그리기\n",
    "sns.histplot(data, color='skyblue')\n",
    "\n",
    "# 그래프에 제목과 축 레이블 추가\n",
    "plt.title('아파트 실거래가 히스토그램')\n",
    "plt.xlabel('아파트 실거래가')\n",
    "plt.ylabel('빈도수')\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d207d2",
   "metadata": {},
   "source": [
    "### 3. 시군구 컬럼 탐색\n",
    "[문제3]\n",
    "'Sigungu' 피처의 값들이 어떻게 구성되어 있는지 확인해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sigungu'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c86f2",
   "metadata": {},
   "source": [
    "### 4. 지번 컬럼 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.['Jibun'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c226e",
   "metadata": {},
   "source": [
    "### 5. apt 컬럼 탐색(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('아파트 종류 총 개수', len(train['apt_name'].unique()))\n",
    "display(train['apt_name'].value_counts()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d189a",
   "metadata": {},
   "source": [
    "### 6. apt 컬럼 탐색(2)\n",
    "[문제4] \n",
    "'apt_name'를 기준으로 groupby 연산을 적용해 'trasaction_real_price'의 평균값 구하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train[['apt_name', 'transaction_rea;_price']].groupby('apt_name').mean()\n",
    "\n",
    "# Seaborn을 사용하여 히스토그램 그리기\n",
    "sns.histplot(data, color='skyblue')\n",
    "\n",
    "# 그래프에 제목과 축 레이블 추가\n",
    "plt.title('아파트 실거래가 히스토그램')\n",
    "plt.xlabel('아파트 실거래가')\n",
    "plt.ylabel('아파트 수')\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a289d",
   "metadata": {},
   "source": [
    "### 7. exclusive_use_area 컬럼 탐색(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train['exclusive_use_area']\n",
    "\n",
    "num_bins = 30 # 구간의 개수\n",
    "\n",
    "plt.figure(figsize=(18,4))\n",
    "sns.histplot(data, color='skyblue', bins=num_bins)\n",
    "\n",
    "plt.title('아파트 실거래가 히스토그램')\n",
    "plt.xlabel('아파트 실거래가')\n",
    "plt.ylabel('빈도수')\n",
    "\n",
    "x_ticks = [min(data) + i * (max(data) - min(data))/num_bins for i in range(num_bins + 1)]\n",
    "plt.xticks(x_ticks)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# bins는 히스토그램을 만들 때, 데이터를 몇개의 구간으로 나눌지를 지정하는 파라미터.\n",
    "\n",
    "# 결과해석\n",
    "# 58~96 제곱미터 넓이의 집의 거래량이 가장 많은 것을 확인할 수 있음.\n",
    "# 그리고 타겟 데이터와 마찬가지로 왼쪽으로 그래프가 쏠려있는 것을 확인할 수 있어,\n",
    "# 두 값이 상관관계가 높은지 확인해볼 필요가 있어 보임. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582b6dd",
   "metadata": {},
   "source": [
    "### 8. exclusive_use_area 컬럼 탐색(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25770877",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train['exclusive_use_area']\n",
    "\n",
    "num_bins = 10 # 구간의 개수\n",
    "\n",
    "plt.figure(figsize=(18,4))\n",
    "sns.histplot(data, color='skyblue', bins=num_bins)\n",
    "\n",
    "plt.title('아파트 실거래가 히스토그램')\n",
    "plt.xlabel('전용면적')\n",
    "plt.ylabel('빈도수')\n",
    "\n",
    "x_ticks = [min(data) + i * (max(data) - min(data) / num_bins for i in range(num_bins + 1 )]\n",
    "plt.xticks(x_ticks)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# x_ticks는 전용면적 최소~최대 범위를 10등분했을 때의 경계값 목록이며, \n",
    "# plt.xticks(x_ticks)로 그 지점에 x축 눈금을 표시하는 역할을 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44383645",
   "metadata": {},
   "source": [
    "### 9. year_of_completion 컬럼 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8decc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yearly = train.groupby(train['year_of_completion'])['transaction_real_price'].mean()\n",
    "\n",
    "plt.plot(train_yearly.index, train_yearly.values)\n",
    "plt.Xlabel('연도')\n",
    "plt.ylabel('가격')\n",
    "plt.title('연도별 아파트 가격')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 결과해석\n",
    "# 준공년도는 1979년부터 2017년까지 다양한 아파트들이 존재.\n",
    "# 일반적으로 다른 조건이 동일하다면, 오래된 아파트는 새 아파트에 비해 가격이 상대적으로 낮음.\n",
    "# 하지만 출력된 그래프를 보면 아파트 가격에 다른 요인들이 더 큰 영향을 미치기 때문에 \n",
    "# 준공년도의 경향성으 찾기 어려워보임.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8d58f",
   "metadata": {},
   "source": [
    "### 10. 날짜 데이터 전처리\n",
    "[문제5]\n",
    "'transaction_day'피처와 'transaction_date'피처를 합치고, 데이터의 형식을 datetime으로 변경하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08647c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tran_data(x):\n",
    "    if type(x) == int:\n",
    "        if x < 10:\n",
    "            return '0'+str(x)\n",
    "        else:\n",
    "            return str(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "train['transaction_day'] = train['transaction_day'].apply(preprocess_tran_date)\n",
    "train['transaction_date'] = train['transaction_year_month'].astype(int).astype(str) + train['transaction_day'].astype(str)\n",
    "train['transaction_date'] = pd.to_datetime(train['transaction_date'])\n",
    "train = train.sort_values('transaction_date').reset_index(drop=True) \n",
    "\n",
    "# astype()은 지정한 데이터 타입으로 변환하는 메서드.\n",
    "# 예를 들어, df['col'] = df['col'].astype('float32')는 'col' 열의 데이터 타입을 float32로 변환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fc422",
   "metadata": {},
   "source": [
    "### 11. 날짜별 타겟 데이터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Scatter Plot 그리기\n",
    "sns.regplot(\n",
    "        x=train['transaction_date'].map(mdates.date2num),\n",
    "        y=train['transaction_real_price'],\n",
    "        scatter_kws={'color': 'skyblue'},\n",
    "        line_kws={'color': 'red'}\n",
    ")\n",
    "\n",
    "# x축과 y축 레이블 설정\n",
    "plt.xlabel('transaction_date')\n",
    "plt.ylabel('Price')\n",
    "\n",
    "# 그래프 제목 설정\n",
    "plt.title('Scatter Plot of Price over Time')\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()\n",
    "\n",
    "# 서울의 아파트 가격은 과거부터 지금까지 꾸준히 오르는 경향성이 있는데,\n",
    "# 정말로 그러한 경향이 있는지 확인해 보기.\n",
    "\n",
    "# 그래프를 확인해 보면, 직선 하나를 확인 할 수 있음.\n",
    "# 직선의 기울기가 양수이므로, 날짜와 가격 사이에 양의 상관관계가 있음을 알 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae07f4",
   "metadata": {},
   "source": [
    "### 12. floor 데이터 탐색 \n",
    "[문제6]\n",
    "빈칸을 채워 'floor'가 1일때와 1이 아닐때의 아파트 시거래가 평균을 구해보자. \n",
    "단, 'sigungu'는 '서울특별시 강남구 대치동'이며, 'exclusive_use_area'는 60보다 큰 경우만 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fa445",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_floor = train['floor'] == 1\n",
    "daechi = train['sigungu'] == '서울특별시 강남구 대치동'\n",
    "area_above_60 = train['exclusive_use_area'] > 60 \n",
    "\n",
    "print('1층 이상: ', train[one_floor & daechi & area_above_60].transaction_real_price.mean())\n",
    "print('2층 이상: ', train[~one_floor & daechi & area_above_60].transaction_real_price.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed58cb8",
   "metadata": {},
   "source": [
    "## Stage3. 데이터 전처리(1)\n",
    "날짜 데이터를 전처리하고, 이를 이용해 아파트 가격을 예측하는데 핵심이 되는 데이터들을 추가하기.\n",
    "특히 아파트 가격은 금리에 영향을 받는 것으로 알려져 있어 이러한 데이터를 추가해주면 예측에 도움이 됨.\n",
    "추가적으로, 최근에 거래된 가격을 추가해 보는 등 다양한 방법으로 날짜 데이터를 사용해 중요한 피처를 추가할 수 있음. \n",
    "\n",
    "목표\n",
    "- 날짜 데이터 전처리\n",
    "- 날짜와 Join할 수 있는 데이터 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c575cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f6f0b3",
   "metadata": {},
   "source": [
    "### 1. 데이터 읽어오기\n",
    "[문제1]\n",
    "빈칸을 채워 train과 test를 하나의 데이터프레임으로 합쳐 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None # 경고 메시지 끄기 # 두개의 데이터를 합치면 인덱스 중복으로 인해 경고 메시지 뜸.\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "interest_rate = pd.read_csv('interest_rate.csv') # 한국은행 기준금리 데이터 불러오기\n",
    "\n",
    "train['train_test'] = 'train'\n",
    "test['train_test'] = 'test' \n",
    "all_data = pd.concat([train, test])\n",
    "all_data = all.data.reset_index(drop=True) # 인덱스 재설정 # 두 데이터를 합친 후 인덱스가 중복되므로 초기화 설정 \n",
    "\n",
    "train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa383cd",
   "metadata": {},
   "source": [
    "### 2. 날짜 데이터\n",
    "[문제2]\n",
    "날짜 관련 데이터가 'transaction_year_month', 'transaction_day'로 분리되어 있는데, \n",
    "'transaction_date'에 합쳐 하나의 피쳐로 만들어 봅시다. \n",
    "하나로 합쳐진 'transaction_date'는 yyyymmdd형식의 문자열(str)로 만들어 줍니다. \n",
    "예를 들어 '20240101'과 같이 만들어 줍니다. \n",
    "\n",
    "그리고 나서 'transaction_date'를 to_datetime()함수를 이용해 datetime 형태로 만들어 줍시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 힌트 'transaction_day' 피처가 10보다 작은 경우에는 앞에 숫자 '0' 붙이기\n",
    "# 데이터프레임에서 덧셈연산을 이용하면 문자열을 연결할 수 있음\n",
    "# datetime은 덧셈, 뺄셈 및 비교 연산이 가능하기 때문에 시계열 데이터를 다룰 때 편리함. 연,월,일 부분 정보 추출도 가능.\n",
    "# datetime형태로 변환하기 위해 pandas의 to_datetime()함수 사용\n",
    "\n",
    "\n",
    "def preprocess_tran_date(x):\n",
    "    ''' \n",
    "    정수형 변수인 transaction_day 피처를 전처리하는 함수 \n",
    "    '''\n",
    "    return  f\"{int(x):02d}\"  # x를 정수(int)로 변환 : 빈자리는 0으로 채우고 두 자릿수의 정수로(decimal) 포맷.\n",
    "\n",
    "# 1. 일(day) 두 자리 문자열로 변환    \n",
    "all_data['transaction_day'] = all_data['transaction_day'].apply(preprocess_tran_date)\n",
    "\n",
    "# 2. 연월 + 일 결합 → '20240105' 형식\n",
    "all_data['transaction_date'] = all_data['transaction_year_month'].astype(int).astype(str) + all_data['transaction_day'].astype(str)\n",
    "# transaction_year_month 컬럼(예:\"202401\")을 astype(int) 정수로 바꿨다가 astype(str)문자로 바꾸기. 왜냐하면 바로 str로 바꾸면 nan값 생겨서 깨질 수 있기 때문. \n",
    "# 그다음 transaction_day 컬럼(예:\"5\")을 astype(str) 문자열로 변환. 위 f\"{int(x):02d}\" 함수로 \"05\" 라는 두자릿수 맞춰진 상태.\n",
    "# + 로 합쳐서 결과적으로 \"20240105\" 라는 문자열로 맞춤. \n",
    "\n",
    "# 3. 문자열 날짜 → datetime 형식\n",
    "all_data['transaction_date'] = pd.to_datetime(all_data['transaction_date']) \n",
    "# pd.to_datetime 으로 문자형(str) \"20240105\"를 진짜 날짜형(datetime64)로 변경 => 날짜 계산, 시계열 정렬, 월연도 추출 가능하도록.\n",
    "\n",
    "# 4. 날짜순으로 정렬 + 인덱스 초기화\n",
    "all_data = all_data.sort_values('transaction_date').reset_index(drop=True)\n",
    "# sort_value: 오름차순 정렬. 내림차순은 ('컬럼명', ascending=False) \n",
    "# # 컬럼별로 오름, 내림차순 지정하고 싶을 떄 : sort_values(by=['컬럼1', '컬럼2'],ascending=[False, True]) \n",
    "# reset_index: 인덱스를 다시 0부터 순서대로 초기화. \n",
    "# drop=True: 기존 인덱스는 버림. (drop=False면 기존 인덱스 옆에 새로 생성됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0311ef",
   "metadata": {},
   "source": [
    "### 3. 아파트 키값 생성\n",
    "[문제3]\n",
    "ngroup() 함수를 이용해 'sigungu','apt_name'을 이용한 그룹별 인덱스 만들어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '시군구+아파트명' 조합별 고유번호(=아파트 ID)을 새로운 컬럼으로 생성하기\n",
    "all_data['apartment_id'] = all_data.groupby(['sigungu','apt_name']).nroup()\n",
    "all_data['apartment_id'] \n",
    "\n",
    "# all_data.groupby(['sigungu','apt_name']): ('강남구','힐스테이트') , ('서초구', '래미안')으로 그룹 생성. \n",
    "# .ngroup(): 각 그룹에 번호 매겨주는 함수. 0부터 시작하는 정수 ID를 부여. 예: ('강남구','힐스테이트')= 0 , ('서초구', '래미안')= 1\n",
    "# all_data['apartment_id'] : 'apartment_id'라는 이름의 컬럼으로 저장하기.\n",
    "# # 이렇게 만든 apartment_id는 나중에 같은 아파트별 평균가격, 거래횟수 집계 또는 모델 학습시 아파트를 카테고리 변수로 인코딩할 때 유용함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbe139",
   "metadata": {},
   "source": [
    "#### 4. 아파트 실거래가 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(string):\n",
    "    if type(string) == str:\n",
    "        string = string.replace(',','')\n",
    "        return int(string)\n",
    "    else:\n",
    "        return string \n",
    "    \n",
    "all_data['transaction_real_price'] = all_data['transaction_real_price'].apply(str_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dc9f6",
   "metadata": {},
   "source": [
    "### 5. 아파트 면적으로 분류하기\n",
    "[문제4] 'exclusive_use_area' 피처에서 파생된 피처를 만들어 봅시다.\n",
    "'bucket_area'피처를 새로 만들어 60 미만인 경우 0, 85 미만인 경우 1, 이외에는 2를 넣어줍시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_area_bucket(area):\n",
    "    '''\n",
    "    exclusive_use_area(전용면적)에 따라 구간(bucket)을 나누는 함수\n",
    "    60 미만 -> 0\n",
    "    85 미만 -> 1\n",
    "    그외 -> 2\n",
    "    '''\n",
    "    \n",
    "    if area < 60:                              # area는 시리즈가 전체가 아니라 한 행의 값 하나씩.\n",
    "    # if all_data['exclusive_use_area'] <= 60 : # 이렇게 쓰면 컬럼 전체를 비교하라는 뜻임.\n",
    "        return 0 \n",
    "        # return all_data['bucket_area'] == 0   # == 비교 연산자는 True/False를 반환. \n",
    "    if area < 85 :\n",
    "        return 1\n",
    "    else :                                      # else 뒤에 : 콜론 빼먹지 말기. \n",
    "        return 2\n",
    "    \n",
    "# 각 행의 exclusive_use_area 값에 함수 적용\n",
    "all_data['bucket_area'] = all_data['exclusive_use_area'].apply(make_area_bucket)\n",
    "\n",
    "# 결과 확인 \n",
    "all_data[['exclusive_use_area','bucket_area']].head()\n",
    "\n",
    "# 함수 안에서 all_data를 참조할 필요 없음\n",
    "# apply는 각 원소(=area 값)을 area로 전달해주기 때문에, 함수 내부에서는 all_data 전체를 쓸 필요가 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da925303",
   "metadata": {},
   "source": [
    "### 6. 최근 아파트 가격 데이터\n",
    "[문제5]\n",
    "for문을 이용해 데이터프레임의 데이터를 처리하려고 합니다.\n",
    "빈칸을 채워 반복문을 완성해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032aed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # 진행상황을 % 로 보여주는 라이브러리 \n",
    "from datetime import datetime \n",
    "\n",
    "def get_recent_price(all_data, idx, row):\n",
    "    # 전체 데이터에서, 현재 인덱스까지의 데이터를 추출한다.\n",
    "    temp_df = all_data.loc[:idx]\n",
    "    \n",
    "    # 추출한 데이터에서, 거래날짜가 row기준으로 과거이고 비슷한 면적인 아파트 거래를 추출.\n",
    "    temp_df = temp_df[\n",
    "        (temp_df['transaction_date'] < row['transaction_date']) &\n",
    "        (temp_df['bucket_area'] == row['bucket_area'])\n",
    "    ]\n",
    "\n",
    "    # 만약 추출한 결과가 아무것도 없으면, 2016-01-01 이전 데이터에서 데이터를 추출.\n",
    "    if len(temp_df) == 0:\n",
    "        temp_df = all_data[\n",
    "         (all_data['transaction_date'] < datetime.strptime('2016-01-01', \"%Y-%m-%d\"))  \n",
    "         (all_data['bucket_area'] == row['bucket_area'])\n",
    "        ]\n",
    "        \n",
    "# datetime 라이브러리\n",
    "# datetime.now() : 현재 날짜와 시간 (연월일시분초) 2025-10-23-10-24-12\n",
    "# datetime.today() : 오늘 날짜와 시간 now()와 비슷\n",
    "# datetime.strptime(\"2024-01-05\",\"%Y-%m-%d\") \n",
    "    # strptime = string parse time : 문자열 시간을 파싱(parse)해서 datetime 객체로 변환한다. \n",
    "    # 문자열 -> 날짜로 변환. \"2024-01-05\" -> datetime(2024,1,5)\n",
    "    \n",
    "# datetime.strftime(\"%Y년 %m월 %d일\") \n",
    "    # strftime = string format time : 시간 데이터 datetime 객체를 지정한 형식(format)의 문자열로 만든다. \n",
    "    # 날짜 -> 문자열로 변환. \"2024년 01월 05일\"\n",
    "    \n",
    "# datetime.timedelta(days=7) : 7일 간격 생성. 오늘부터 7일 후의 날짜 -> today + timedelta(days=7) \n",
    "    \n",
    "    \n",
    "    # 추출한 데이터 중, 같은 아파트인 경우 해당 값을 추출.\n",
    "    recent_price = temp_df[(temp_df['apartment_id'] == row['apartment_id'])]\n",
    "\n",
    "    if len(recent_price) == 0:\n",
    "        # 만약 같은 이름의 아파트가 존재하지 않으면, 같은 시궁구에 있는 아파틑 추출.\n",
    "        recent_price = temp_df[(temp_df['sigungu'] == row['sigungu'])]\n",
    "        recent_price = recent_price.iloc[-1]['transaction_real_price']\n",
    "    else:\n",
    "         # 만약 같은 시군구에 아파트 거래내역이 존재하지 않으면, 가장 최근 거래를 사용.\n",
    "        recent_price = recent_price.iloc[-1]['transaction_rea_price'] \n",
    "    \n",
    "    # 가장 최근 거래가 없으면 전체 평균을 사용.\n",
    "    if recent_price is None:\n",
    "        recent_price = temp_df['transaction_real_price'].mean()\n",
    "    \n",
    "    return recent_price\n",
    "\n",
    "for idx, row in tqdm(all_data.iterrows(), total=all_data.shape[0]):  # iterrow()는 (idx, row)를 쌍으로 반복(iteration)할 수 있게 해주는 메서드. \n",
    "    if row['train_test'] == 'test':\n",
    "        continue \n",
    "    all_data.loc[idx, 'recent_price'] = get_recent_price(all_data, idx, row)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58b8ca",
   "metadata": {},
   "source": [
    "## 7. 아파트 최근 거래량\n",
    "[문제6] 빈칸을 채워, 거래일자 기준으로 90일 이전까지의 데이터를 추출해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta \n",
    "\n",
    "for idx, row in tqdm(all_date.iterrows(), total = all_data.shape[0]):\n",
    "    # transaction_date가 2014-03-30 날짜 이전 데이터인 경우, 2014-03-30 ~ 2014-01-01 데이터를 추출.\n",
    "    if row['transaction_date'] <= datetime.strptime('2014-03-30', \"%Y-%m-%d\") : \n",
    "        start_day = datetime.strptime('2014-03-30', \"%Y-%m-%d\")\n",
    "        end_day = datetime.strptime('2014-01-01', \"%Y-%m-%d\")\n",
    "        cnt = len(all_data[(all_data['transaction_date'] >= start_day) & (all_data['transaction_date'] < end_Dat) & (all_date['sigungu'] == row['sigungu'])])\n",
    "    \n",
    "    # 거래날짜를 기준으로 3개월 이전 데이터를 추출해 봅시다.\n",
    "    else:\n",
    "        start_day = row['transacction_date'] - timedelta(days=90)\n",
    "        end_day = row['transaction_date']\n",
    "        cnt = len(all_data[(all_data['transaction_date'] >= start_day) & (all_data['transaction_date'] < end_day) & (all_data['sigungu'] == row['sigungu'])])\n",
    "        \n",
    "    all_data.loc[idx, 'transaction_cnt'] = cnt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8901d69",
   "metadata": {},
   "source": [
    "### 8. 금리 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca97bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rate = pd.read_csv('interest_rate.csv') # 한국은행 기준금리 \n",
    "\n",
    "def make_date(row):\n",
    "    month_day = row['월일'].replace('월','-')\n",
    "    month_dat = month_day.replace('dlf','')\n",
    "    date = str(row['연도']) + '-' + month_day\n",
    "    return date\n",
    "\n",
    "interst_rate['날짜'] = interst_rate.apply(lambda x: make_date(x), axis=1)\n",
    "interest_rate['날짜'] = pd.to_datetime(interest_rate['날짜'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d59ba",
   "metadata": {},
   "source": [
    "### 9. 금리 데이터 적용하기\n",
    "한국은행_기준금리.csv 파일에는 금리가 변동된 날짜 기준으로 데이터가 쌓여 있기 때문에\n",
    "아파트 매매 날짜 기준으로 최근 갱신된 금리를 적용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9caf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(all_data.iterrows(), total = all_data.shape[0]):\n",
    "    date = row['transaction_date']\n",
    "    rate = interest_rate[interest_rate['날짜'] <= date].iloc[0]['금리']\n",
    "    # 기준금리 데이터에서 \"거래일(date) 이전의 금리만\" 필터링해서 interest_rate라는 새로운 df 생성.\n",
    "    # iloc[0] 그 결과에서 첫번째 행. ['금리'] 컬럼 값 가져와라. \n",
    "    all_data.loc[idx, 'interest_rate'] = rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb93a0",
   "metadata": {},
   "source": [
    "### 10. 연월 변수 추가\n",
    "연도별 월별 영향을 파악하기 위해 데이터 추가\n",
    "이를 통해 연도별 전체적인 경향성과 월별로 반복되는 경향성을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f306f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['transaction_year'] = all_data['transaction_date'].dt.year  # dt는 datetime64 형식 데이터 accessor(접근자) # 날짜가 들어있는 컬럼에 .dt를 붙여 날짜 속성을 .으로 꺼내 쓸 수 있다. \n",
    "all_data['transaction_month'] = all_data['transaction_date'].dt.month "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b7c69",
   "metadata": {},
   "source": [
    "### 데이터 합치기 \n",
    "🔹 merge → “기준 열로 조인하는 가로 결합 (JOIN)”  \n",
    "🔹 concat → “같은 구조 데이터를 단순히 이어붙이는 세로 결합 (UNION ALL과 유사)”  \n",
    "🔹 union → “concat과 비슷하지만 중복을 제거하는 세로 결합 (SQL UNION과 동일)”\n",
    "| 기능         | 공통점           | 차이점                              |\n",
    "| ---------- | ------------- | -------------------------------- |\n",
    "| **merge**  | DataFrame 합치기 | 공통 컬럼(key) 기준으로 **가로 결합 (JOIN)** |\n",
    "| **concat** | DataFrame 합치기 | 단순히 **세로로 쌓기** (중복 유지)           |\n",
    "| **union**  | DataFrame 합치기 | **세로로 쌓고 중복 제거** (SQL UNION과 동일) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae797dd",
   "metadata": {},
   "source": [
    "## Stage4. 데이터 전처리(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136aa5d2",
   "metadata": {},
   "source": [
    "### 도입\n",
    "이번 스테이지에서는 아파트에 대한 정보를 정리하는 작업을 진행할 것입니다.  \n",
    "가지고 있는 데이터에는 아파트의 전용면적, 아파트 이름, 그리고 층수가 포함되어 있습니다.   \n",
    "이를 이용해 아파트 실거래가를 예측할 수 있는 적합한 형태의 데이터로 변환할 예정입니다. \n",
    "\n",
    "### 목표\n",
    "- 아파트 이름을 이용해 가격이 비슷한 단지 끼리 묶기\n",
    "- 아파트 세금 혜택을 기준으로 전용면적 데이터 전처리\n",
    "- 층수에 따라 인기가 적은 매물을 반영해 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dc872",
   "metadata": {},
   "source": [
    "### 1. 데이터 분석 전 준비\n",
    "[문제1] 빈칸을 채워 데이터프레임의 인덱스를 초기화해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fe863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd. options.mode.chained_assignment = None # 연쇄할당 경고 숨기는 코드 # 데이터 합칠때 경고 발생 띄우지 말고 조용히 실행해. ('Warn'-경고표시, 'None'-경고끄기, 'Raise'-경고 대신 에러로 중단)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "train['train_test'] = 'train'\n",
    "test['train_test'] = 'test'\n",
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 경고 숨기는 options.mode.chained_assignment = None 대신 \n",
    "# loc[] 사용해서 좀 더 명확히 표현하는 법. \n",
    "# pandas가 복사본을 수정하는 건지 혼동하지 않도록. 원본 수정이라고 명시.\n",
    "\n",
    "# .loc을 사용하여 명확히 원본의 특정 열을 수정\n",
    "# train.loc[:, 'train_test'] = 'train'\n",
    "# test.loc[:, 'train_test'] = 'test'\n",
    "\n",
    "# 두 데이터프레임 결합 (행 방향)\n",
    "# all_data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "all_data.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b150b3",
   "metadata": {},
   "source": [
    "### 2. 아파트 이름 전처리(1)\n",
    "[문제2] 빈칸을 채워 kmeans 변수에 K-means 클러스터링 모델을 선언해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4500a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np \n",
    "\n",
    "# 평균값을 구할 때에는 train 데이터에서 구한다.\n",
    "train = all_data[all_data['train_test'] == 'train']\n",
    "\n",
    "# 아파트 실거래가 수치형 변수로 변환\n",
    "train['transaction_real_price'] = train['transaction_real_price'].str.replace(',','').astype('int')\n",
    "\n",
    "# 아파트 이름을 기준으로 평균 실거래가 구하기\n",
    "data = train[['apt_name', 'transaction_real_price']] \n",
    "data = data.groupby('apt_name').mean()\n",
    "arr = data['transaction_real_price'].to_numpy().reshape(-1,1) \n",
    "# to_numpy 통해 순수한 숫자 배열로 변환. 계산 용이하도록. # reshape(-1,1)은 1차원 -> 2차원 배열 (n,1)형태로 만들어 머신러닝 모델 입력 형태로 변환.\n",
    "# (n,) : 1차원 -> 모델이 인식 못함. [35000, 42000, 39000]\n",
    "# (n, 1) : 2차원 -> 인식 가능. [[35000], [42000], [39000]]\n",
    "# (-1,1) 의미는 -1: 행 개수를 자동으로 계산, 1: 열이 1개짜리 2차월 배열로 만들라는 뜻. \n",
    "\n",
    "# 가격을 기준으로 아파트 군집화\n",
    "k = 5\n",
    "kmeans = KMeans(n_cluster=k, n_init=10)\n",
    "kmeans.fit(arr)\n",
    "\n",
    "# 가격을 기준으로 군집의 순서를 정렬하기 위해 인덱스를 추출\n",
    "sort_order = np.argsort(kmeans.cluster_centers_.flatten()) # np.argsort() 정렬된 순서의 인덱스를 반환. \n",
    "# flatten()은 1차원 한줄로 펴주는 함수. 인덱스 셀때 유용하기 때문에 같이 쓰임. \n",
    "# 내림차순은 np.argsort()[::-1] 또는 np.argsort(-arr) \n",
    "\n",
    "# 군집화 결과를 가격 순서대로 재할당\n",
    "labels = np.zeros_like(kmeans.labels_)\n",
    "for i, cluster in enumerate(sort_order):\n",
    "    labels[kmeans.labels_ == cluster] = i \n",
    "    \n",
    "# 군집화 결과와 가격을 데이터에 추가\n",
    "data['cluste'] = labels\n",
    "data = data.reset_indec()\n",
    "data = data[['apart_name', 'cluster']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9abb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e11b2dfe",
   "metadata": {},
   "source": [
    "### 3. 아파트 이름 전처리(2)\n",
    "[문제3] 빈칸을 채워 위에서 구한 아파트의 분류 결과를 'all_data'에 'apt_name'을 기준으로 left join 연산을 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, data, how='left', left_on='apt_name', right_on='apt_name')\n",
    "\n",
    "cluster_mode = all_data.loc[all_data['train_test'] == 'train', 'cluster'].mode()\n",
    "all_data['cluster'] = all_data['cluster'].fillna(cluster_mode)\n",
    "\n",
    "all_data['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74285b2",
   "metadata": {},
   "source": [
    "### 4. 아파트 면적 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_area_bucket(area):\n",
    "    if area < 60: #59타입\n",
    "        return 0\n",
    "    elif area < 85: #84타입\n",
    "        return 1\n",
    "    else:\n",
    "        return 2 \n",
    "    \n",
    "all_data['bucket_area']= all_data['exclusive_use_area'].apply(make_area_bucket)\n",
    "all_data['bucket_area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef479e69",
   "metadata": {},
   "source": [
    "### 5. 층수 전처리\n",
    "[문제4] 빈칸을 채워 floor 피처를 활용해 전처리하는 코드를 작성해 봅시다.\n",
    "\n",
    "아파트 층수는 저층일 경우, 일반적으로 가격이 낮습니다.  \n",
    "그렇기 때문에 3층 이하의 층수는 따로 분류해 전처리 하도록 해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e21154",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['low_floor'] = all_data['floor'].apply(lambda x: 0 if x <= 3 else 1)\n",
    "all_data['low_floor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24763c0c",
   "metadata": {},
   "source": [
    "## Stage5. 모델 학습 및 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778b053",
   "metadata": {},
   "source": [
    "### 1.도입\n",
    "앞에서 전처리한 데이터들을 이용해 모델을 학습해 보겠습니다.\n",
    "어떤 데이터가 얼마나 실거래가 예측에 도움이 되는지 검증할 차례입니다.\n",
    "추가적으로 데이터가 시간 순서대로 구성되어 있기 때문에, 그에 맞게 학습 데이터와 검증 데이터를 나눠 검증을 진행해 보겠습니다.\n",
    "\n",
    "### 2. 목표\n",
    "- 모델 학습 및 적정한 피처 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e905a9",
   "metadata": {},
   "source": [
    "### 1. 데이터 분석 전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a59114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sampled_submission.csv')\n",
    "interest_rate = pd.read_Csv('interest_rate.csv')\n",
    "\n",
    "train['train_test'] = 'train'\n",
    "test['train_test'] = 'test'\n",
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a510fc",
   "metadata": {},
   "source": [
    "### 2. 가격 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd350d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(string):\n",
    "    if type(string) == str:\n",
    "        string = string.replace(',','')\n",
    "        return int(string)\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "all_data['trransaction_real_price'] = all_data['transaction_real_price'].apply(str_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb7402",
   "metadata": {},
   "source": [
    "### 3. 아파트 키값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['apartment_id'] = all_data.roupby(['sigungu', 'apt_name']).ngroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaeb57a",
   "metadata": {},
   "source": [
    "### 4. 날짜 데이터 전처리\n",
    "[문제1] 빈칸을 채워 all_data를 transaction_Date를 기준으로 정렬하세요,   \n",
    "그리고 인덱스(index)를 리셋(reset)해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tran_date(x):\n",
    "    if type(x) === int:\n",
    "        if x < 10:\n",
    "            return '0'+str(x)\n",
    "        else:\n",
    "            return str(x)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "# 간단 한줄 함수 : return f\"{int(x):02d}\n",
    "\n",
    "all_data['transaction_day'] = all_data['transaction_day'].apply(preprocess_tran_date)\n",
    "all_data['transaction_date'] = all_data['transaction_year_month'].astype(int).astype(str)+all_data['transaction_day'].astype(str)\n",
    "all_data['transaction_date'] = pd.to_datetime(all_data['transaction_date'])\n",
    "all_data = all_data.sort_values('transaction_date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc109a",
   "metadata": {},
   "source": [
    "| 키워드               | 어떤 데이터    | 결과         | 기억하기      |\n",
    "| ----------------- | --------- | ---------- | --------- |\n",
    "| **sort_values()** | DataFrame | 정렬된 데이터 반환 | SQL ORDER BY 개념|\n",
    "| **np.sort()**     | NumPy 배열  | 정렬된 값      | “값 정렬”    |\n",
    "| **np.argsort()**  | NumPy 배열  | 정렬 순서 인덱스  | “인덱스값 반환”   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a52453",
   "metadata": {},
   "source": [
    "### 5. 최근에 거래된 가격 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49cefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def make_area_bucket(area):\n",
    "    if area < 60: # 59타입\n",
    "        return 0\n",
    "    elif area < 85: # 84타입\n",
    "        return 1\n",
    "    else:\n",
    "        return 2 \n",
    "    \n",
    "# 아파트 면적 전처리\n",
    "all_data['bucket_area'] = all_data['exclusive_use_area'].apply(make_area_bucket)\n",
    "\n",
    "def get_recent_price(idx, all_data):\n",
    "    temp_df = all_data.loc[:idx]\n",
    "    temp_df = temp_df[temp_df\n",
    "        (temp_df['transaction_date'] < row['transaction_date']) &\n",
    "        (temp_df['bucket_area'] == row['bucket_area'])\n",
    "    ]\n",
    "    if len(temp_df) == 0:\n",
    "        temp_df = all_data[ \n",
    "            (all_data['transaction_date'] < datetime.strptime('2016-01-01', '%Y-%m-%d')) &\n",
    "            (all_data['bucket_area'] == row['bucket_area']\n",
    "        )]\n",
    "    \n",
    "    # 아파트 아이디 같은 것 찾기\n",
    "    recent_price = temp_df[(temp_df['apartment_id'] == row['apartment_id'])]\n",
    "    if len(recent_price) == 0:\n",
    "        recent_price = temp_df[(temp_df['sigungu'] == row['sigungu'])]\n",
    "        recent_price = recent_price.iloc[-1]['transaction_real_price']\n",
    "    else:\n",
    "        recent_price = recent_price.iloc[-1]['transaction_real_price']\n",
    "        \n",
    "    if recent_price is None:\n",
    "        recent_price = temp_df['transaction_real_price'].mean() # 2019년 전체평균\n",
    "        \n",
    "    return recent_price\n",
    "for idx, row in tqdm(all_data.iterrows(), total = all_data.shape[0]):\n",
    "    if row['train_test'] == 'test' :\n",
    "        continue \n",
    "    all_data.loc[idx, 'recent_price'] = get_recent_price(idx, all_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaaabb9",
   "metadata": {},
   "source": [
    "### 6. 아파트의 최근 거래량 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fbc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "for idx, row in tqdm(all_data.iterrows(), total = all_data.shape[0]):\n",
    "    # transaction_date가 2014-03-30 날짜 이전 데이터인 경우, 2014-03-30 ~ 2014-01-01 데이터를 추출합니다. \n",
    "    if row['trandaction_date'] <= datetime.strptime('2014-03-30', \"%Y-%m-%d\"): \n",
    "        start_day = datetime.strptime('20214-03-30', \"%Y-%m-%d\")\n",
    "        end_day = datetime..strptime('2014-01-01', \"%Y-%m-%d\")\n",
    "        cnt = len(all_data[(all_data['transaction_date'] >= start_day) & (all_data['transaction_date'] < end_day) & (all_data['sigungu'] == row['sigungu'])]) \n",
    "        \n",
    "    # 거래날짜를 기준으로 3개월 이전 데이터를 추출해 봅시다.\n",
    "    else:\n",
    "        start_day = row['transaction_date'] - timedelta(days=90)\n",
    "        end_day = row['transaction_date']\n",
    "        cnt = len(all_data[(all_data['transaction_date'] >= start_day) & (all_data['transaction_date'] < end_day) & (all_data['sigungu'] == row['sigungu'])])\n",
    "    \n",
    "    all_data.loc[idx, 'transaction_cnt'] = cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc9f6a",
   "metadata": {},
   "source": [
    "### 7. 금리 데이터 추가하기 \n",
    "[문제2]\n",
    "문자열 형식의 날짜 데이터를, datetime 형태로 변경해 봅시다.  \n",
    "먼저 날짜 데이터를 '2023-12-01' 형태로 변경해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interest_rate에 들어있는 날짜 데이터는 금리가 변동된 날짜. \n",
    "# 그렇기 때문에 거래일자 기준 최근 금리가 변동된 날짜의 금리를 사용하면 도미.\n",
    "\n",
    "# 금리 변동일자 데이터를 전처리해 봅시다. pd.to_datetime()함수에는 '2023-01-01'형태의 문자열로 넣어야 함.\n",
    "# '월일'을 '01월01일' -> '01-01'로 변경 \n",
    "\n",
    "def make_date(row):\n",
    "    '''\n",
    "    \"연도'와 '월일' 컬럼을 조합해 YYYY-MM-DD 문자열로 변환해야 함\n",
    "    (ex. '2023', '1월13일' -> '2023-01-13')\n",
    "    '''\n",
    "    year = str(row['연도'])\n",
    "    \n",
    "    # '월', '일' 제거, 공백 제거\n",
    "    monthday = str(row['월일']).replace('월','').replace('dlf','').strip()\n",
    "    \n",
    "    # 공백 기준으로 분리 -> ['1', '13'] 형태\n",
    "    parts = monthday.split()\n",
    "    if len(parts) == 2:\n",
    "        month = parts[0].zfill(2) # zfill(n) 왼쪽에 0 추가해서 문자열 전체 길이를 n으로 맞춤. zero fill 0으로 채운다는 의미.\n",
    "        day = parts[1].zfill(2)\n",
    "    else:\n",
    "        # 혹시 데이터가 '0113' 형태일 수도 있으니 대비\n",
    "        month = monthday[:2].zfill(2)\n",
    "        day = monthday[2:].zfill(2) \n",
    "    return f\"{year}-{month}-{day}\" \n",
    "    \n",
    "interest_rate['날짜'] = interest_rate.apply(lambda x: make_date(x), axis=1) # axis=0 세로 방향 각 열(colums)에 함수 적용, axis=1 가로 방향 각 행(row)에 함수 적용\n",
    "interest_rate['날짜'] = pd.to_datetime(interest_rate['날짜']\n",
    "interest_rate['날짜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffbec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜에 맞게 금리를 적용해 줍시다.\n",
    "for idx, row in tqdm(all.iterrows(), total = all_data.shape[0]):\n",
    "    date = row['transaction_date']\n",
    "    rate = interest_rate[interest_rate['날짜'] <= date].iloc[0]['금리']\n",
    "    all_data.loc[idx, 'interest_rate'] = rate \n",
    "\n",
    "# 연월 데이터 추가\n",
    "all_data['transaction_year'] = all_data['transaction_date'].dt.year\n",
    "all_data['transaction_month'] = all_data['transaction_date'].dt.month "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291d20f",
   "metadata": {},
   "source": [
    "### 8. 아파트 이름 데이터 전처리\n",
    "[문제2] 아파트 이름(apt_name)별 아파트 실거래가(transaction_real_price)의 평균을 구해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "import numpy as np \n",
    "\n",
    "# 아파트 별로 가격 평균값 구하기\n",
    "train = all_data[all_data['tran_test'] == 'train']\n",
    "data = train[['apt_name', 'transaction_real_price']]\n",
    "\n",
    "data = data.groupby('apt_name').mean()\n",
    "arr = data['transaction_rea;_price'].to_numpy().reshape(-1,1)\n",
    "\n",
    "# 가격을 기준으로 아파트 군집화\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "kmeans.fit(arr)\n",
    "\n",
    "# 가격을 기준으로 군집의 순서를 정렬하기 위해 인덱스를 추출\n",
    "sort_order = np.argsort(kmeans.cluster_centers_.flatten())\n",
    "\n",
    "# 군집화 결과를 가격 순서대로 재할당\n",
    "labels = np.zeros_like(kmeans.labels_)\n",
    "for i, cluster in enumerate(sort_order):\n",
    "    labels[kemeans.labels_==cluster] = i\n",
    "    \n",
    "# 군집화 결과와 가격을 데이터에 추가\n",
    "data['cluster'] = labels\n",
    "data = data.reset_index()\n",
    "data = data[['apt_name', 'cluster']]\n",
    "\n",
    "all_data = pd.merge(all_data, data, how='left', left_on='apt_name', right_on='apt_name')\n",
    "\n",
    "cluster_mode = all_data.loc[all_data['train_test'] == 'train', 'cluste'].mean()[0]\n",
    "all_data['cluster'] = all_data['cluster'].fillna(cluster_mode)\n",
    "\n",
    "all_data['cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbc354",
   "metadata": {},
   "source": [
    "### 9. 층수 전처리\n",
    "[문제3] lambda 함수를 이용해 'floor'피처가 3층 이하인 경우 0,  \n",
    "이외의 경우는 1인 새로운 피처를 만들어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14959b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['low_floor'] = all_data['floor'].apply(lambda x: 0 if x <= 3 else 1)\n",
    "all_date['low_floor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87514612",
   "metadata": {},
   "source": [
    "### 10. 모델 학습 및 검증\n",
    "[문제4] 빈칸을 채워 옵투나를 이용해 하이퍼파라미터를 최적화해 봅시다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7203f3",
   "metadata": {},
   "source": [
    "이번 문제에서는 랜덤포레스트 모델을 이용해 모델을 만들겠습니다.  \n",
    "그리고 하이퍼파라미터를 최적화 하기 위해 옵투나(Optuna) 라이브러리를 사용해 봅시다.  \n",
    "베이즈 최적화 알고리즘을 기반으로 탐색을 수행합니다.  \n",
    "초기에느 하이퍼파라미터들의 랜덤한 조합을 선택하고, 목적 함수를 이용해 모델을 평가합니다.  \n",
    "이후 평가된 결과를 바탕으로 더 나은 성능이 기대되는 하이퍼파라미터 영역으로 점진적으로 탐색하며, 더 나은 조합을 찾아냅니다.   \n",
    "이 과정을 반복하여 최적의 하이퍼파라미터 조합을 찾습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc25887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandaomForesttRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna \n",
    "\n",
    "validation_year = 2022\n",
    "\n",
    "columns = [\n",
    "    'year_of_completion', 'recent_price', 'transaction_cnt', 'interest_rate', 'transaction_year',\n",
    "    'transaction_month', 'cluster', 'bucket_area', 'low_floor', \n",
    "]\n",
    "\n",
    "train_x = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] < validation_year), columns] \n",
    "train_y = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'], 'transaction_real_price']\n",
    "                       \n",
    "val_x = all.data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year), columns]\n",
    "val_y = a;;_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year), 'transaction_real_price'] \n",
    "\n",
    "def objective(trial) : \n",
    "    # 하이퍼파라미터 탐색 대상\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    \n",
    "    # RandomForestRegressor 모델 학습\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    # 검증 데이터로 평가\n",
    "    y_pred = model.predict(val_x)\n",
    "    mse = mean_squared_error(val_y, y_pred)\n",
    "    return mse \n",
    "\n",
    "# Optuna를 사용하여 하이퍼파라미터 탐색\n",
    "study = optuna.create_study(direction='minimize') # 목표는 최소화 \n",
    "    # direction='minimize' → 손실 함수(loss, RMSE 등) 최소화 \n",
    "    # direction='maximize' → 정확도(accuracy, F1 등) 최대화\n",
    "study.optimize(objective, n_traials=50) # 50회 반복하여 탐색\n",
    "\n",
    "# 최적의 하이퍼파라미터 값 출력\n",
    "best_params = study.best_params\n",
    "print(\"Best Params:\", best_params)\n",
    "\n",
    "\n",
    "# 실행하면 날짜 시분초, Trial 1 부터 50까지 진행과정 리스트 뜨고 \n",
    "# \"Best Params: {'n_estimator':100, 'max_depth':5}\" 출력됨. 베스트 파라미터 값 확인."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cb8f3",
   "metadata": {},
   "source": [
    "🔹 n_estimators — 트리(tree)의 개수\n",
    "\n",
    "모델이 몇 개의 결정트리(Decision Tree)를 만들어서 예측할지를 결정.  \n",
    "너무 적으면 학습 부족(underfitting), 너무 많으면 과적합(overfitting) 위험  \n",
    "  \n",
    "🔹*max_depth (또는 depth) — 각 트리의 최대 깊이\n",
    "\n",
    "한 트리가 얼마나 “깊게” 분기할지를 결정.  \n",
    "트리의 “질문 단계”가 몇 번까지 가능한지를 정하는 것  \n",
    "(예: “가격이 1억 이상인가?” → “서울인가?” → “84㎡ 이상인가?” …)\n",
    "\n",
    "✅ 정리 요약\n",
    "| 파라미터           | 의미                 | 조절 효과                |\n",
    "| -------------- | ------------------ | -------------------- |\n",
    "| `n_estimators` | 모델이 학습할 **트리의 개수** | 많을수록 성능↑, 과적합/속도↓ 주의 |\n",
    "| `max_depth`    | 각 트리의 **깊이(복잡도)**  | 깊을수록 복잡, 과적합 위험      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bd92d",
   "metadata": {},
   "source": [
    "### 11. 모델 학습\n",
    "[문제5] 빈칸을 채워 예측한 결과를 MAE로 평가해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델 재학습\n",
    "best_model = RandomForestRegressor(n_estimators=best_params['n_estimators'], \n",
    "                                   max_depth=best_params['max_depth'])\n",
    "best_model.fit(train_x, train_y)\n",
    "\n",
    "# 예측\n",
    "pred_val_ls = best_model.predict(val_x)\n",
    "\n",
    "mae = maean_absolute_error(val_y, pred_val_ls)\n",
    "print(mae) \n",
    "\n",
    "# mean_absolute_error()\n",
    "# → 실제값(val_y)과 예측값(pred_val_ls)의 차이(오차)의 절댓값 평균을 계산하는 함수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b0c39",
   "metadata": {},
   "source": [
    "| 항목           | 현재 상황                        | 해석                |\n",
    "| ------------ | ---------------------------- | ----------------- |\n",
    "| MAE = 26,349 | 단위가 **만원**이라면 → 약 2.6억 오차    | ❌ 매우 큼 (모델 개선 필요) |\n",
    "|              | 단위가 **원**이라면 → 약 26,000원 오차  | ✅ 매우 양호           |\n",
    "|              | 데이터가 스케일링(normalized) 되어 있다면 | ⚠️ 복원 후 재평가 필요    |\n",
    "\n",
    "MAE = 26349가 큰지 작은지는 타깃의 단위와 범위에 따라 다릅니다.  \n",
    "만약 실거래가(만원 단위)를 예측 중이라면 오차 2~3억 수준으로 높은 편이므로  \n",
    "모델 성능 개선이 필요합니다 (특징 추가, 스케일링, 하이퍼파라미터 조정 등).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a6528",
   "metadata": {},
   "source": [
    "## 12. 피처 중요도 확인\n",
    "[문제6] 위에서 학습한 랜덤포레스트(Random Forest) 모델에서, 피처 중요도를 추출해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "feature_names = train_x.colounms\n",
    "\n",
    "# 피처 중요도에 따라 내림차순으로 인덱스를 정렬\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# 피처 이름을 중요도 순서에 맞게 재배열 \n",
    "sorted_feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "# 피처 중요도 시각화 \n",
    "plt.figure()\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.barh(range(train_x.shape[1]), importances[indices], align=\"center\") \n",
    "plt.yticks(range(train_x.shape[1], sorted_feature_name)\n",
    "plt.ylabel(\"Features\") \n",
    "plt.xlabel(\"Importance\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c2236",
   "metadata": {},
   "source": [
    "### 13. 중요 피처(Feature) 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd06718",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['bucket_area', 'low_floor', 'year_of_completion']\n",
    "\n",
    "train_filtered_x = train_x.drop(columns=drop_columns)\n",
    "val_filtered_x = val_x.drop(columns=drop_columns)\n",
    "\n",
    "best_model = RandomForestRegressor(n_estimators=best_params['n_estimators'],\n",
    "                                   max_depth=best_params['max_depth'])\n",
    "best_model.fit(train_filtered_x, train_y) \n",
    "\n",
    "# 예측\n",
    "pred_vsl_ls = best_model.predict(val_fitered_x)\n",
    "mae = mean_absolute_error(pred_val_ls, val_y)\n",
    "print(mae)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df4c60",
   "metadata": {},
   "source": [
    "### 14. 시계열 모델 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f425ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for validation_year in [2018,2019,2020,2021,2022]:\n",
    "    columns = [\n",
    "        'recent_price', 'transaction_cnt', 'interest_rate', 'transaction_year', 'transaction_month', 'cluster',\n",
    "    ]\n",
    "    train_x = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] < validation_year), columns]\n",
    "    train_y = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] < validation_year), 'transaction_real_price']\n",
    "    \n",
    "    val_x = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year), columns]\n",
    "    val_y = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year), 'transaction_real_price']\n",
    "    \n",
    "# 랜덤 포레스트 모델 생성 및 훈련\n",
    "model_trial = RandomForestRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth']\n",
    ")\n",
    "model_trial.fit(train_x, train_y)\n",
    "\n",
    "# 예측\n",
    "pred_val_ls = model_trial.predict(val_x)\n",
    "mae = mean_absolute_error(pred_val_ls, val_y)\n",
    "print(validation_year, '년도 MAE:', mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5b513",
   "metadata": {},
   "source": [
    "### 15. 테스트 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f933a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "columns = [\n",
    "    'recent_price', 'transaction_cnt', 'interest_rate', 'transaction_year', 'transaction_month', 'cluster'\n",
    "]\n",
    "train_x = all_data.loc[all_datap['train_test'] == 'train', columns]\n",
    "train_y = all_data.loc[all_data['train_test'] == 'train', 'transaction_real_price']\n",
    "test_x = all_data.loc[all_data['train_test'] == 'test', columns]\n",
    "\n",
    "# 랜덤 포레스트 모델 생성 및 훈련\n",
    "model = RandomForestRegressor()\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# 예측\n",
    "pred_ls = list()\n",
    "now_df = all_data.loc[all_data['train_test'] == 'train']\n",
    "test = all_data.loc[all_data['train_test'] == 'test']\n",
    "\n",
    "for idx, row in tqdm(test.iterrows(), total = test.shape[0]):\n",
    "    now_df = pd.cincat([now_df, test.loc[[idc]]])\n",
    "    test_x.loc[idx, 'recent_price'] = get_recent_price(idx, now_df)\n",
    "    \n",
    "    pred = model.predict(test_x.loc[idx:idx]) \n",
    "    \n",
    "    now_df.loc[idx, 'transaction_real_price'] = pred\n",
    "    pred_ls.append(pred[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b0351",
   "metadata": {},
   "source": [
    "### 16. 정답 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['transaction_real_price'] = pred_ls\n",
    "submission.to_csv('submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c1806",
   "metadata": {},
   "source": [
    "## Stage6. 모델 고도화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f28ff",
   "metadata": {},
   "source": [
    "앞에서 전처리한 데이터들을 이용해 모델을 학습해 보겠습니다.\n",
    "\n",
    "목표: 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83e552",
   "metadata": {},
   "source": [
    "### 1. 데이터 분석 전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78084ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.option.mode.chained_assignement = None \n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.csv('sample_submission.csv')\n",
    "interest_rate = pd.read_csv('interest_rate.csv')\n",
    "\n",
    "train['train_test'] = 'train'\n",
    "test['train_test'] = 'test'\n",
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50cdec0",
   "metadata": {},
   "source": [
    "### 2. 가격 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c089f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(string):\n",
    "    if type(string) == str:\n",
    "        string = string.replace(',','')\n",
    "        return int(string)\n",
    "    else:\n",
    "        return string\n",
    "all_data['transaction_real_price'] = all_data['transation_real_price'].apply(str_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196a49",
   "metadata": {},
   "source": [
    "### 3. 날짜 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cde554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tran_date(x):\n",
    "    if type(x) == int:\n",
    "        if x < 10:\n",
    "            return '0'+str(x)\n",
    "        else:\n",
    "            return str(x)\n",
    "    else:\n",
    "        return x \n",
    "\n",
    "all_data['transaction_day'] = all_data['transaction_day'].apply(preprocess_tran_date)\n",
    "all_data['transaction_date'] = all_data['transaction_year_month'].astype(int).astype(str) + all_data['transaction_day'].astype(str)\n",
    "all_data['transaction_date'] = pd.to_datetime(all_data['transaction_date'])\n",
    "all_data = all_data.sort_values('transaction_date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac6664",
   "metadata": {},
   "source": [
    "### 4. 최근에 거래된 가격 구하기\n",
    "부동산에서 아파트의 시장 가격을 평가할 때, 최근에 거래된 아파트의 가격을 기준으로 매물을 평가하는 경우가 많습니다.   \n",
    "  \n",
    "그렇기 때문에 get_recent_price() 함수를 만들어서, 아파트의 최근 가격을 구해 봅시다.   \n",
    "아파트의 최근 가격은 아파트가 거래된 과거 데이터에서 찾습니다.   \n",
    "  \n",
    "1. 전체 데이터에서, 현재 인덱스까지의 데이터를 추출한다.  \n",
    "2. 추출한 데이터에서, 거래날짜가 row 기준으로 과거이고 비슷한 면적인 아파트 거래를 추출한다.  \n",
    "3. 만약 추출한 결과가 없으면, 2016-01-01 이전 데이터에서 데이터를 추출한다.  \n",
    "4. 추출한 데이터 중, 같은 아파트인 경우 해당 값을 추출한다.  \n",
    "5. 만약 같은 이름의 아파트가 존재하지 않으면, 같은 시군구에 있는 아파트를 추출한다.  \n",
    "6. 만약 같은 시구군에 아파트 거래내역이 존재하지 않으면, 가장 최근 거래를 사용한다.  \n",
    "7. 가장 최근 거래가 없으면 전체 평균을 사용한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee41a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def make_area_bucket(area):\n",
    "    if area < 60: # 59타입\n",
    "        return 0\n",
    "    elif area < 85: #84타입\n",
    "        return 1\n",
    "    else:\n",
    "        return 2 \n",
    "\n",
    "# 아파트 면적 전처리\n",
    "all_data['bucket_area'] = all_data['exclusive_use_area'].apply(make_area_bucket)\n",
    "\n",
    "# 아파트 아이디 생성\n",
    "all_data['apartment_id'] = all_data.groupby(['sigungu', 'apt_name']).ngroup()\n",
    "\n",
    "def get_recent_price(all_data, idx, row):\n",
    "    # 전체 데이터에서, 현재 이전 인덱스까지의 데이터를 추출한다.\n",
    "    if idx >= 1:\n",
    "        index = idx -1\n",
    "    else:\n",
    "        index = idx\n",
    "\n",
    "    # 전체 데이터에서, 현재 인덱스까지의 데이터를 추출.\n",
    "    temp_df = all_data.loc[:index]\n",
    "\n",
    "    # 추출한 데이터에서, 거래날짜가 row기준으로 과거이고 비슷한 면적인 아파트 거래를 추출.\n",
    "    tempt_df = temp_df[\n",
    "        (temp_df['transaction_date'] < row['transaction_date']) & \n",
    "        (temp_df['bucket_area'] == row['bucket_area'])\n",
    "    ]\n",
    "\n",
    "    # 만약 추출한 결과가 아무것도 없으면, 2026-01-01 이전 데이터에서 데이터를 추출. \n",
    "    if len(temp_df) == 0:\n",
    "        temp_df = all_data[\n",
    "            (all_data[''])\n",
    "        ]    \n",
    "    \n",
    "    # 추출한 데이터 중, 같은 아파트인 경우 해당 값을 추출.\n",
    "    recent_price = temp_df[(ttemp_df['apartment_id'] == row['apartment_id'])]\n",
    "\n",
    "    if len(recent_price) == 0:\n",
    "        # 만약 같은 이름의 아파트가 존재하지 않으면, 같은 시군ㄱ구에 있는 아파트를 추출.\n",
    "        recent_price = temp_df[(temp_df['sigungu'] == row['sigungu'])]\n",
    "        recent_price = recent_price.iloc[-1]['transaction_real_price'] \n",
    "    else:\n",
    "        # 만약 같은 시군구에 아파트 거래내역이 존재하지 않으면, 가장 최근 거래를 사용.\n",
    "        recent_price = recent_price.iloc[-1]['transaction_real_price']\n",
    "\n",
    "    # 가장 최근 거래가 없으면 전체 평균을 사용.\n",
    "    if recent_price is None:\n",
    "        recent_price = temp_df['transaction_real_price'].mean()\n",
    "    \n",
    "    return recent_price \n",
    "\n",
    "for idx, row in tqdm(all_data.iterrows(), total = all.data.shape[0]):\n",
    "    if row['train_test'] == 'test':\n",
    "        continue\n",
    "    all_data.loc[idx, 'recent_price'] = get_recent_price(all_data, idx, row) \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8787e",
   "metadata": {},
   "source": [
    "### 5. 아파트 최근 거래량 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "for idx, row in tqdm(all_data.iterrows(), total = all_data.shape[0]):\n",
    "    # transaction_date가 2014-03-30 날짜 이전 데이터인 경우, 2014-03-30 ~ 2014-01-01 데이터를 추출.\n",
    "    if row['transaction_data'] <= datetime.strptime('2014-03-30', \"%Y-%m-%d\"):\n",
    "        start_day = datetime.strptime('2014-03-30', \"%Y-%m-%d\")\n",
    "        end_dat = datetime.strptime('2014-01-01', \"%Y-%m-%d\")\n",
    "        cnt = len(all_data[(all_data['transaction_date'] >= start_day) & (all_data['transaction_date'] < end_day) & (all_data['sigungu'] == row['sigungu'])])\n",
    "        \n",
    "    # 거래날짜를 기준으로 3개월 이전 데이터를 추출해 봅시다.\n",
    "    else:\n",
    "        start_day = row['transaction_date'] - timedelta(days=90) \n",
    "        end_day = row['transaction_date']\n",
    "        cnt = len(all_data[(all_data['transaction_date'] >= start_day) & (all_data['transaction_date'] < end_day) & (all_data['sigungu'] == row['sigungu'])])\n",
    "        \n",
    "    all_data.loc[idx, 'transaction_cnt'] = cnt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84006e30",
   "metadata": {},
   "source": [
    "### 6. 금리 데이터 추가하기\n",
    "[문제1] Datetime 유형인 데이터 'transaction_date'에서 'year', 'month' 데이터를 추출해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918aaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_date(row):\n",
    "    month_day = row['월일'].replace('월 ','-')\n",
    "    month_day = month_day.replace('일', '')\n",
    "    date = str(row['연도'])+ '-' + month_day \n",
    "    return date\n",
    "\n",
    "# 날짜 데이터 yyyy-mm-dd 형태로 변경\n",
    "interest_rate['날짜'] = interest_rate.applt(lambda x: make_date(x), axis=1) \n",
    "\n",
    "# 날짜를 datetime 유형으로 변경\n",
    "interest_rate['날짜'] = pd.to_datetime(interest_rate['날짜'])\n",
    "\n",
    "for idx, row in tqdm(all_data.iterrow(), total = all_data.shape[0]):\n",
    "    date = row['transaction_date']\n",
    "    rate = interest_rate[interest_rate['날짜'] <= date].oloc[0]['금리']\n",
    "    all_data.loc[idx, 'interest_rate'] = rate\n",
    "\n",
    "# 연월 데이터 추가\n",
    "all_data['transaction_year'] = all_data['transaction_date'].year # 반드시 .dt.year  # datetime은 DataFrame이 아니라 자료형(type)이라 .dt 없이 인덱싱 불가 \n",
    "all_data['transaction_month'] = all_data['transaction_date'].month # .dt.month 로 출력.  # 메서드 함수가 아닌 속성이기 때문에 () 생략 ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbc2e1",
   "metadata": {},
   "source": [
    "### 7. 아파트 이름 데이터 전처리\n",
    "[문제2] 빈칸을 채워 가격을 기준으로 K-means 클러스터링을 이용해 아파트를 군집화해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np \n",
    "\n",
    "# 아파트 별로 가격 평균값 구하기\n",
    "train = all_data[all_data['train_test'] == 'train']\n",
    "data = train[['apt_name', 'transaction_real_price']]\n",
    "\n",
    "data = data.groupby('apt_name').mean()\n",
    "arr = data['transaction_real_price'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# 가격을 기준으로 아파트 군집화\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "kmeans.fit(arr)\n",
    "\n",
    "# 가격을 기준으로 군집의 순서를 정렬하기 위해 인덱스를 추출 \n",
    "sort_order = np.argsort(kmeans.cluster_centers.flatten())\n",
    "\n",
    "# 군집화 결과를 가격 순서대로 재할당\n",
    "labels = np.zeros_like(kmeans.labels_)\n",
    "for i, cluster in enumerate(sort_order):\n",
    "    labels[kmeans.labels_ == cluster] = i\n",
    "    \n",
    "# 군집화 결과와 가격을 데이터에 추가\n",
    "data['cluster'] = labels\n",
    "data = data.reset_index()\n",
    "data = data[['apt_name', 'cluster']]\n",
    "\n",
    "all_data = pd.merge(all_data, data, how='left', left_on='apt_name', right_on='apt_name')\n",
    "\n",
    "cluster_mode = all_data.loc[all_data['train_test'] == 'train', 'cluster'].mode()[0]\n",
    "all_data['cluster'] = all_data['cluster'].fillna(cluster_mode)\n",
    "\n",
    "all_data['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8469ee",
   "metadata": {},
   "source": [
    "## 8. 랜덤포레스트 모델 학습\n",
    "[문제3] objective 메소드 이용해 옵튜나 실행 내용을 정의하고, 최적의 하이퍼파라미터를 찾아 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d25d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna \n",
    "\n",
    "validation_year = 2022 \n",
    "\n",
    "columns = [\n",
    "    'recent_price', 'transaction_cnt', 'interest_rate', 'transaction_year', 'transaction_month', 'cluster'\n",
    "]\n",
    "\n",
    "train_x = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] < validation_year), columns]\n",
    "train_y = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] < validation_year), 'transaction_real_price']\n",
    "\n",
    "val_x = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year), columns]\n",
    "val_y = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year, 'transaction_real_price']\n",
    "                     \n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 탐색 대상\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
    "    max_depth = trial.suggest_int('mac_dapth', 2, 32)\n",
    "    \n",
    "    # 랜덤포레스트 모델 학습\n",
    "    model = RandomForestRegressor(n_estimators, max_deppth=max_depth, random_state=7)\n",
    "    model.fit(train_x, train_y) \n",
    "    \n",
    "    # 검증 데이터로 평가\n",
    "    y_pred = model.predict(val_x)\n",
    "    mse = mean_squared_error(vla_y, y_pred)\n",
    "    return mse \n",
    "    \n",
    "# Optuna를 사용하여 하이퍼파라미터 탐색\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params_rf = study.best_params\n",
    "print(\"Best Params:\", best_params_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dc558",
   "metadata": {},
   "source": [
    "### 9. Xgboost 모델 학습\n",
    "[문제4]\n",
    "예측문제에 사용하는 xgboost 모델을 만들어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea36337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 탐색 대상\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
    "    max_depth = trial.sugget_int('max_depth', 2, 32)\n",
    "    learning_rate = trial.suggest_loungeform('learning_rate', 0.001, 0.1)\n",
    "    \n",
    "    # xgboost 모델 학습\n",
    "    model = xgb.XGBRegressor(n_estimators=n_estimators,\n",
    "                             max_depth=max_dapth,\n",
    "                             learning_rate = learning_rate,\n",
    "                             random_state=7)\n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    # 검증 데이터로 평가 \n",
    "    y_pred = model.predict(val_x)\n",
    "    mse = mean_squared_error(val_y, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Optuna를 사용하여 하이퍼파라미터 탐색\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params_xgb =study.best_params \n",
    "print(\"Best Params:\", best_params_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee6e41",
   "metadata": {},
   "source": [
    "## 10. 모델 교차검증\n",
    "[문제5] 랜덤포레스트의 예측값과 XGBoost의 예측값의 평균을 구해 예측값을 계산해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63511205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for validation_year in [2018, 2019, 2020, 2021, 2022]\n",
    "columns = [\n",
    "    'recent_price', 'transaction_cnt', 'interest_rate', 'transaction_year', 'transaction_month', 'cluster',\n",
    "]\n",
    "train_x = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] < validation_year), columns]\n",
    "train_y = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] < validation_year), 'transaction_real_price']\n",
    "\n",
    "val_x = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year), columns]\n",
    "val_y = all_data.loc[(all_data['train_test'] == 'train') & (all_data['transaction_year'] == validation_year), 'transaction_real_price'] \n",
    "\n",
    "# 모델 생성 및 훈련\n",
    "model_rf = RandomForestRegressor(n_estimators=best_params_rf['n_estimators'], \n",
    "                                 max_depth=best_params_rf['max_depth'],\n",
    "                                 random_state=7\n",
    "                                 )\n",
    "model_rf.fit(train_x, train_y)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=best_params_xgb['n_estimators'], \n",
    "                             max_depth=best_params_xgb['max_depth'],\n",
    "                             learning_rate=best_params_xgb['learning_rate'],\n",
    "                             random_state=7)\n",
    "\n",
    "model_xgb.fit(train_x, train_y)\n",
    "\n",
    "# 예측\n",
    "pred_rf_ls = model_rf.predict(val_x)\n",
    "pred_xgb_ls = model_xgb.predict(val_x)\n",
    "blended_prediction = (pred_rf_ls + pred_xgb_ls) /2 \n",
    "\n",
    "mae = mean_absolute_error(blended_prediction, val_y)\n",
    "print(validation_year, '년도 MAE:' , mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d41db",
   "metadata": {},
   "source": [
    "## 11. 테스트 데이터 예측\n",
    "[문제6] 빈칸을 채워 테스트 데이터를 예측하는 코드를 작성해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58852d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "columns = [\n",
    "    'recent_price', 'transaction_cnt', 'interest_rate', 'transaction_year', 'transaction_month', 'cluste', \n",
    "]\n",
    "train_x = all_data.loc[all_data['train_test'] == 'train', columns]\n",
    "train_y = all_data.loc[all_data['train_test'] == 'train', 'transaction_real_price'] \n",
    "test_x = all_data.loc[all_data['train_test'] == 'test', columns] \n",
    "\n",
    "# 모델 생성 및 훈련\n",
    "model_rf = RandomForestRegressor(n_estimators=best_params_rf['n_estimators'],\n",
    "                                 max_depth=best_params_rf['max_depth'],\n",
    "                                 random_state=7\n",
    "                                )  \n",
    "model_rf.fit(train_x, train_y)\n",
    "\n",
    "model_xgb = xgb.XGBRgressor(n_estimators=best_params_xgb['n_estimators'],\n",
    "                            max_depth=best_params_xgb['max_depth'],\n",
    "                            learning_rate=best_params_xgb['learning_rate'],\n",
    "                            random_state=7)\n",
    "model_xgb.fit(train_x, train_y)\n",
    "\n",
    "# 예측\n",
    "pred_ls = list()\n",
    "now_df = all_data.loc[all_data['train_test'] == 'train']\n",
    "test = all_data.loc[all_data['train_test'] == 'test']\n",
    "\n",
    "for idx, row in tqdm(test.iterrows(), total = test.shape[0]):\n",
    "    now_df = pd.concat([now_df, test.loc[[idx]]]) \n",
    "    test_x.loc[idx, 'rescent_price'] = get_recent_price(now_df, idx, row)\n",
    "    \n",
    "    # 예측\n",
    "    pred_rf_ls = model_rf.predict(test_x.loc[idx:idx])\n",
    "    pred_xgb_ls = model_xgb.predict(test_x.loc[idx:idx])\n",
    "    blended_prediction = (pred_rf_ls + pred_xgb_ls) / 2\n",
    "    \n",
    "    now_df.loc[idx, 'transaction_real_price'] = blended_prediction\n",
    "    pred_ls.append(blended_prediction[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003291b",
   "metadata": {},
   "source": [
    "## 12. 정답 제출 파일 생성\n",
    "[문제7] 예측한 결과가 들어있는 pred_ls를 submission['transaction_real_price']에 넣어주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7800718",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['transaction_real_price'] = pred_ls\n",
    "submission.to_csv('submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a78ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
